#!/usr/bin/env python3

#  xcapture-bpf -- Always-on profiling of Linux thread activity, by Tanel Poder [https://tanelpoder.com]
#  Copyright 2024 Tanel Poder
#
#  This program is free software; you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License along
#  with this program; if not, write to the Free Software Foundation, Inc.,
#  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
#
#  SPDX-License-Identifier: GPL-2.0-or-later

__version__      = "2.0.2"
__author__       = "Tanel Poder"
__date__         = "2024-06-27"
__description__  = "Always-on profiling of Linux thread activity using eBPF."
__url__          = "https://0x.tools"

DEFAULT_PATH     = "." # where to write xcapture -o output files
DEFAULT_GROUP_BY = "st,username,comm,syscall,cmdline" # for xtop mode
DECODE_CHARSET   = "utf-8"

import os, sys, pwd, time, ctypes, platform, distro, re, shutil, argparse, signal
from collections import defaultdict
from datetime import datetime
from bcc import BPF, PerfType, PerfSWConfig

# all available fields (if you add more fields to thread_state_t in C, add them here)
available_fields = [ 'timestamp', 'st', 'tid', 'pid', 'username', 'comm', 'comm2', 'syscall'
                   , 'cmdline', 'cmdline2', 'offcpu_u', 'offcpu_k', 'oncpu_u', 'oncpu_k'
                   , 'waker_tid', 'sch' ]

# default output fields for ungrouped full detail output
output_fields = [ 'timestamp', 'st', 'tid', 'pid', 'username', 'comm', 'syscall', 'cmdline'
                 , 'offcpu_u', 'offcpu_k', 'oncpu_u', 'oncpu_k', 'waker_tid', 'sch' ]


# syscall id to name translation (todo: fix aarch64 include file lookup)
def extract_system_call_ids(unistd_64_fh):
    syscall_id_to_name = {}

    # strip 3264bit prefixes from syscall names
    for name_prefix in ['__NR_', '__NR3264_']:
        for line in unistd_64_fh.readlines():
            tokens = line.split()
            if tokens and len(tokens) == 3 and tokens[0] == '#define' and tokens[2].isnumeric() is True:
                _, s_name, s_id = tokens
                s_id = int(s_id)
                if s_name.startswith(name_prefix):
                    s_name = s_name[len(name_prefix):]
                    syscall_id_to_name[s_id] = s_name

    return syscall_id_to_name

def get_system_call_names():
    psn_dir=os.path.dirname(os.path.realpath(__file__))
    kernel_ver=platform.release().split('-')[0]

    # this probably needs to be improved for better platform support
    if platform.machine() == 'aarch64':
        unistd_64_paths = ['/usr/include/asm-generic/unistd.h']
    else:
        unistd_64_paths = [  '/usr/include/asm/unistd_64.h', '/usr/include/x86_64-linux-gnu/asm/unistd_64.h'
                           , '/usr/include/asm-x86_64/unistd.h', '/usr/include/asm/unistd.h'
                           , psn_dir+'/syscall_64_'+kernel_ver+'.h', psn_dir+'/syscall_64.h']
    
    for path in unistd_64_paths:
        try:
            with open(path) as f:
                return extract_system_call_ids(f)
        except IOError as e:
            pass

    raise Exception('unistd_64.h not found in' + ' or '.join(unistd_64_paths) + '.\n' +
                    '           You may need to "dnf install kernel-headers" or "apt-get install libc6-dev"\n') 

# syscall lookup table
syscall_id_to_name = get_system_call_names()


# task states
TASK_RUNNING           =   0x00000000
TASK_INTERRUPTIBLE     =   0x00000001
TASK_UNINTERRUPTIBLE   =   0x00000002
TASK_STOPPED           =   0x00000004
TASK_TRACED            =   0x00000008

EXIT_DEAD              =   0x00000010
EXIT_ZOMBIE            =   0x00000020
EXIT_TRACE             =   (EXIT_ZOMBIE | EXIT_DEAD)

TASK_PARKED            =   0x00000040
TASK_DEAD              =   0x00000080
TASK_WAKEKILL          =   0x00000100
TASK_WAKING            =   0x00000200
TASK_NOLOAD            =   0x00000400
TASK_NEW               =   0x00000800
TASK_RTLOCK_WAIT       =   0x00001000
TASK_FREEZABLE         =   0x00002000
TASK_FREEZABLE_UNSAFE  =   0x00004000 # depends on: IS_ENABLED(CONFIG_LOCKDEP)
TASK_FROZEN            =   0x00008000
TASK_STATE_MAX         =   0x00010000 # as of linux kernel 6.9

##define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWPN"

task_states = {
    0x00000000: "R", # "RUNNING",
    0x00000001: "S", # "INTERRUPTIBLE",
    0x00000002: "D", # UNINTERRUPTIBLE",
    0x00000004: "T", # "STOPPED",
    0x00000008: "t", # "TRACED",
    0x00000010: "X", # "EXIT_DEAD",
    0x00000020: "Z", # "EXIT_ZOMBIE",
    0x00000040: "P", # "PARKED",
    0x00000080: "dd",# "DEAD",
    0x00000100: "wk",# "WAKEKILL",
    0x00000200: "wg",# "WAKING",
    0x00000400: "I", # "NOLOAD",
    0x00000800: "N", # "NEW",
    0x00001000: "rt",# "RTLOCK_WAIT",
    0x00002000: "fe",# "FREEZABLE",
    0x00004000: "fu",# "__TASK_FREEZABLE_UNSAFE = (0x00004000 * IS_ENABLED(CONFIG_LOCKDEP))"
    0x00008000: "fo",# "FROZEN"
}


def get_task_state_name(task_state):
    if task_state == 0:
        return "R"
    if task_state & TASK_NOLOAD: # idle kthread waiting for work
        return "I"

    names = []
    for state, name in task_states.items():
        if task_state & state:
            names.append(name)

    return "+".join(names)
    

# translate uid to username
def get_username(uid):
    try:
        username = pwd.getpwuid(uid).pw_name
        return username
    except KeyError:
        return str(uid)



def print_fields(rows, columns, linelimit=0):
    columns = [col.rstrip() for col in columns] # strip as colname might have extra spaces passed in for width/formatting
    col_widths = {}
    # column width auto-sizing
    for col in columns:
        col_length = len(col) # the col may have extra trailing spaces as a formatting directive
        max_value_length = max((len(str(row[col])) for row in rows if col in row), default=0)
        col_widths[col] = max(col_length, max_value_length)

    header1 = "=== Active Threads "
    header2 = " | ".join(f"{col:<{col_widths[col]}}" for col in columns)

    print(header1 + "=" * (len(header2) - len(header1)) + "\n")
    print(header2)
    print("-" * len(header2))

    #for row in rows:
    for i, row in enumerate(rows):
        line = " | ".join(
            f"{row[col]:>{col_widths[col]}.2f}" if col in ["seconds", "samples", "avg_thr"] 
                                                else f"{str(row[col]):<{col_widths[col]}}"
            if col in row else ' ' * col_widths[col] for col in columns
        )
        print(line)

        # dont break out if linelimit is at its default 0
        if linelimit and i >= linelimit:
            break



def get_ustack_traces(ustack_traces, strip_args=True):
    exclusions = ['__GI___clone3']
    lines = []

    for stack_id, pid in output_ustack:
        if stack_id and stack_id >= 0:   # todo: find why we have Null/none stackids in this map
            line = f"ustack {stack_id:6} "
            stack = list(ustack_traces.walk(stack_id))
            for stack_item in stack: # reversed(stack):
                #print("%d %x %x %d %d %s" % (stack_item.status, stack_item.ip, stack_item.offset, pid, stack_id, str(stack_item.build_id)))
                func_name = b.sym(stack_item, pid).decode(DECODE_CHARSET, 'replace')
                if func_name not in exclusions:
                    if strip_args:
                        func_name = re.split('[<(]', func_name)[0]
                    line += "->" + (func_name if func_name != '[unknown]' else '{:x}'.format(stack_item.ip)) # ip - offset?
                    #line += "->" + (func_name if func_name != '[unknown]' else '{:x}'.format(addr))

            lines.append(line)

    return lines

def get_kstack_traces(kstack_traces):
    exclusions = ['entry_SYSCALL_64_after_hwframe', 'do_syscall_64', 'x64_sys_call', 'ret_from_fork_asm', 'ret_from_fork'
                 , '__bpf_trace_sched_switch', '__traceiter_sched_switch']
    lines = []
 
    for k, v in kstack_traces.items():
        stack_id = k.value
        if stack_id in output_kstack:
            line = f"kstack {stack_id:6} "
            if stack_id >= 0:
                stack = list(kstack_traces.walk(stack_id))
                for addr in reversed(stack):
                    func = b.ksym(addr).decode(DECODE_CHARSET, 'replace')
                    if func not in exclusions and not func.startswith('bpf_'):
                        line += "->" + b.ksym(addr).decode(DECODE_CHARSET, 'replace')

                lines.append(line)
        
    return lines


def pivot_stack_traces(traces):
    pivoted_traces = []
    for trace in traces:
        parts = trace.split("->")
        pivoted_traces.append(parts)
    
    max_length = max(len(trace) for trace in pivoted_traces)
    for trace in pivoted_traces:
        while len(trace) < max_length:
            trace.append("")
    
    return pivoted_traces

def calculate_columns(pivoted_traces, max_line_length):
    max_length = max(len(part) for trace in pivoted_traces for part in trace)
    return max(1, max_line_length // (max_length + 3))

def print_pivoted_dynamic(traces, max_line_length):
    num_traces = len(traces)
    start = 0
    
    while start < num_traces:
        end = start + 1
        while end <= num_traces:
            subset_traces = traces[start:end]
            pivoted_traces = pivot_stack_traces(subset_traces)
            num_columns = calculate_columns(pivoted_traces, max_line_length)
            
            if num_columns < end - start:
                break
            
            end += 1

        end -= 1
        subset_traces = traces[start:end]
        pivoted_traces = pivot_stack_traces(subset_traces)
        
        max_length = max(len(part) for trace in pivoted_traces for part in trace)
        
        print("-" * max_line_length)
        for row in zip(*pivoted_traces):
            print(" | ".join(f"{part:<{max_length}}" for part in row) + ' |')
        
        start = end

# stack printing and formatting choice driver function
def print_stacks_if_nerdmode():
    if args.giant_nerd_mode:
        # printing stacktiles first, so the task state info is in the bottom of terminal output
        (term_width, term_height) = shutil.get_terminal_size()

        print_pivoted_dynamic(get_kstack_traces(stackmap), max_line_length=term_width)
        print()

        print_pivoted_dynamic(get_ustack_traces(stackmap), max_line_length=term_width)
        print()

    if args.nerd_mode:
        # print stack as long line traces
        for s in get_kstack_traces(stackmap): print(s)
        for s in get_ustack_traces(stackmap): print(s)

# group by for reporting
def group_by(records, column_names, sample_attempts_in_set, time_range_in_set):
    grouped_data = defaultdict(lambda: {'samples': 0})

    for record in records:
        key = tuple(record[col] for col in column_names)
        if key not in grouped_data:
            grouped_data[key].update({col: record[col] for col in column_names})
        grouped_data[key]['samples'] += 1

    total_records = len(records)
    grouped_list = list(grouped_data.values())

    for item in grouped_list:
        item['avg_thr'] = round(item['samples'] / sample_attempts_in_set, 2)
        item['seconds'] = round(item['samples'] * (time_range_in_set / sample_attempts_in_set), 2)

    return grouped_list


# main()
signal.signal(signal.SIGPIPE, signal.SIG_DFL)

# args 
parser = argparse.ArgumentParser(description=__description__)
parser.add_argument('-x', '--xtop', action='store_true', help='Run in aggregated top-thread-activity (xtop) mode')
parser.add_argument('-d', dest="report_seconds", metavar='report_seconds', type=int, default=5, help='xtop report printing interval (default: %(default)ds)')
parser.add_argument('-f', '--sample-hz', default=20, type=int, help='sampling frequency in Hz (default: %(default)d)')
parser.add_argument('-g', '--group-by', metavar='csv-columns', default=DEFAULT_GROUP_BY, help='Full column list what to group by')
parser.add_argument('-G', '--append-group-by', metavar='append-csv-columns', default=None, help='List of additional columns to default cols what to group by')
parser.add_argument('-n', '--nerd-mode', action='store_true', help='Print out relevant stack traces as wide output lines')
parser.add_argument('-N', '--giant-nerd-mode', action='store_true', help='Print out relevant stack traces as stacktiles')
parser.add_argument('-c', '--clear-screen', action='store_true', help='Clear screen before printing next output')
parser.add_argument('-w', '--wide', action='store_true', help='Wide raw formatted output of all samples and their fields')
parser.add_argument('-V', '--version', action='version', version=f"%(prog)s {__version__} by {__author__} [{__url__}]", help='Show the program version and exit')
parser.add_argument('-o', '--output-dir', type=str, default=DEFAULT_PATH, help=f'Directory path where to write the output CSV files, default is {DEFAULT_PATH}')
parser.add_argument('-l', '--list', default=None, action='store_true', help='list all available columns for display and grouping')

args = parser.parse_args()

if args.list:
    for f in available_fields:
        print(f)
    sys.exit(0)

# handle xtop -g and -G group by columns
groupby_fields = args.group_by.split(',')
groupby_fields = groupby_fields + args.append_group_by.split(',') if args.append_group_by else groupby_fields

if set(groupby_fields) - set(available_fields):
    print("Error: incorrect group by field name specified, use --list option see allowed columns")
    exit(1)

# eBPF programs have be loaded as root
if os.geteuid() != 0:
    print("Error: you need to run this command as root")
    sys.exit(1)

# ready to go
progname = "xtop" if args.xtop else "xcapture-bpf"
kernname = platform.release().split('-')[0]
archname = platform.machine()
print(f'=== [0x.tools] {progname} {__version__} BETA by {__author__}. {distro.id().title()} Linux {distro.version()} {kernname} {archname}')

# open and load the BPF instrumenter
with open('xcapture-bpf.c', 'r') as file:
    bpf_text = file.read()

print('===  Loading BPF...')
b = BPF(text=bpf_text)

print('===  Loading modules...')
b.add_module('/lib64/libc.so.6')
b.add_module('/lib64/librt.so.1')
b.add_module('/lib64/libaio.so.1')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/bin/oracle')
b.add_module('/lib64/libdl.so.2')
b.add_module('/lib64/libm.so.6')
b.add_module('/lib64/libpthread.so.0')
b.add_module('/lib64/libresolv.so.2')
b.add_module('/lib64/libgcc_s.so.1')
b.add_module('/lib64/libc.so.6')
b.add_module('/lib/ld-linux-aarch64.so.1')
b.add_module('/usr/lib64/libnuma.so.1')
b.add_module('/lib64/libnss_files.so.2')

b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libofs.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libcell19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libskgxp19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libskjcx19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libclsra19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libdbcfg19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libhasgen19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libskgxn2.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libocr19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libocrb19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libocrutl19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libons.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libshpkarm64neon19.so')
b.add_module('/u01/app/oracle/product/19.0.0/dbhome_1/lib/libnque19.so')


# Software CPU_CLOCK is useful in cloud & VM environments where perf hardware events 
# are not available, but software clocks don't measure what happens when CPUs are in 
# critical sections when most interrupts are disabled
b.attach_perf_event(ev_type=PerfType.SOFTWARE, ev_config=PerfSWConfig.CPU_CLOCK
                    , fn_name="update_cpu_stack_profile", sample_freq=1)

# get own pid so to not display it in output
mypid = os.getpid()
print(f"===  Ready (mypid {mypid})\n")

# start sampling the Task State Array
tsa = b.get_table("tsa")
stackmap = b.get_table("stackmap")

# replace digits in "comm" for better grouping and reporting
trim_comm = re.compile(r'\d+')


first_report_printed = False # show first xtop report quicker

while True:
    output_kstack = {} # map of stack_ids seen so far
    output_ustack = {}
    output_records = []

    sample_start = time.time()
    duration = (args.report_seconds if args.xtop and first_report_printed else 1)
    sample_end = sample_start + duration # todo: 1 Hz for raw/csv output for now
    first_report_printed = True
    samples_attempted = 0 # not all TSA samples contain active threads of interest, this tells us how many samples we really took

    while time.time() < sample_end:
        samples_attempted += 1
        ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
        i = tsa.items()[0]

        for i in tsa.items():
            save_record = True
            # extract python values from BPF ctypes, return '-' if there's no match
            fields_dict = {field[0]: getattr(i[1], field[0], '-') for field in i[1]._fields_}

            if fields_dict['tid'] == mypid:
                continue

            # additional fields for adding human readable info (not using None as it would be printed out as "None")
            fields_dict['st']          = ''
            fields_dict['sch']         = '' # for scheduler nerds
            fields_dict['state_flags'] = '' # full scheduler state bitmap
            fields_dict['username']    = ''
            fields_dict['syscall']     = ''
            fields_dict['comm2']       = ''
            fields_dict['cmdline2']    = ''
 
            current_syscall   = syscall_id_to_name.get(fields_dict['syscall_id'], '-') if fields_dict['syscall_set'] else '-'
            in_sched_migrate  = fields_dict['in_sched_migrate']
            in_sched_wakeup   = fields_dict['in_sched_wakeup']
            in_sched_waking   = fields_dict['in_sched_waking']
            is_running_on_cpu = fields_dict['is_running_on_cpu']

            enriched_fields = {"timestamp": ts[:-3]}

            for field_name in fields_dict:
                outv = None # enriched value

                if field_name in ['state', 'st']:
                    state_suffix = ''
                    state = get_task_state_name(fields_dict['state'])
                    # for scheduling latency/preempted tasks (RQ)
                    if state == 'R' and not is_running_on_cpu:
                        state_suffix = 'Q'
                    if state in ['R', 'D'] or (current_syscall == 'io_getevents' and str(fields_dict['comm'], DECODE_CHARSET).startswith('ora')):
                          # or current_syscall == 'semtimedop' and str(fields_dict['comm'], DECODE_CHARSET).startswith('oracle'): # oracle example
                        outv = state + state_suffix
                    else:
                        save_record = False
                        break

                elif field_name.startswith('comm'):
                    val = fields_dict['comm'] # source field is "comm" regardless of potential comm2 output field name
                    if isinstance(val, bytes):
                        outv = str(val, DECODE_CHARSET)
                    else:
                        outv = str(val)
                    if field_name == 'comm':  # only trim "comm", but not comm2 that is the unaltered string
                        outv = re.sub(trim_comm, '*', outv)

                elif field_name.startswith('cmdline'):
                    val = fields_dict['cmdline']
                    if isinstance(val, bytes):
                        outv = str(val, DECODE_CHARSET)
                    else:
                        outv = str(val)
                    if field_name == 'cmdline':
                        outv = re.sub(trim_comm, '*', outv)

                elif field_name == 'syscall':
                    outv = current_syscall

                elif field_name == 'username':
                    outv = get_username(fields_dict['uid']) 

                elif field_name.endswith('_k'):   # kstack id
                    val = fields_dict[field_name]
                    outv = val if val else '-'
                    output_kstack[val] = True

                elif field_name.endswith("_u"):   # ustack id
                    val = fields_dict[field_name]
                    outv = val if val else '-'
                    # using pid/tgid here, address space is same for all threads in a process
                    output_ustack[val, fields_dict['pid']] = True  

                elif field_name == 'sch': 
                    # (in_sched_waking, in_sched_wakeup, is_running_on_cpu)
                    outv  = '-' if in_sched_migrate  else '_'
                    outv += '-' if in_sched_waking   else '_'
                    outv += '-' if in_sched_wakeup   else '_'
                    outv += '-' if is_running_on_cpu else '_'

                else:
                    val = fields_dict[field_name]
                    if isinstance(val, bytes):
                        outv = str(val, DECODE_CHARSET)
                    else:
                        outv = str(val)
                    
                enriched_fields[field_name] = outv

            if save_record:
                output_records.append(enriched_fields)

        time.sleep(1 / (args.sample_hz if args.xtop else 1))


    if output_records:
        if args.clear_screen:
            os.system('clear')

        print_stacks_if_nerdmode()
        print()
 
        if args.xtop:
            # a new field "samples" shows up (count(*))
            grouped_list = group_by(output_records, groupby_fields, samples_attempted, sample_end - sample_start) 
            ordered_aggr = sorted(grouped_list, key=lambda x: x['samples'], reverse=True)
            print_fields(ordered_aggr, ['seconds', 'avg_thr'] + groupby_fields, linelimit=25)

            print()
            print()
            print(f'sampled: {samples_attempted} times')
            print(f'start: {ts[:19]}, duration: {duration}s')
       
            # terminal size may change over time 
            (term_width, term_height) = shutil.get_terminal_size()

            for x in range(1, term_height - len(ordered_aggr) - 10): # header/footer lines
                print()


        else: # wide raw output
            print_fields(output_records, output_fields) 
        
        print("\n")

# That's all, folks!
