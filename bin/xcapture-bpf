#!/usr/bin/env python3

#  xcapture-bpf -- Always-on profiling of Linux thread activity, by Tanel Poder [https://tanelpoder.com]
#  Copyright 2024 Tanel Poder
#
#  This program is free software; you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License along
#  with this program; if not, write to the Free Software Foundation, Inc.,
#  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
#
#  SPDX-License-Identifier: GPL-2.0-or-later

__version__      = "2.0.0"
__author__       = "Tanel Poder"
__date__         = "2024-06-25"
__description__  = "Always-on profiling of Linux thread activity using eBPF."
__url__          = "https://0x.tools"

DEFAULT_PATH     = "." # where to write xcapture -o output files
DEFAULT_GROUP_BY = "st,username,comm" # for xtop mode
DECODE_CHARSET   = "ascii"


import os, sys, pwd, time, ctypes, platform, re, shutil, argparse, signal
from collections import defaultdict
from datetime import datetime
from bcc import BPF, PerfType, PerfSWConfig

# default output fields for ungrouped full detail output
output_fields = [ 'timestamp', 'st', 'flags', 'tid', 'pid', 'username', 'comm', 'syscall' 
                 , 'offcpu_u', 'offcpu_k', 'oncpu_u', 'oncpu_k' 
                 , 'in_sched_waking', 'in_sched_wakeup', 'waker_tid', 'sch' ]


# syscall id to name translation (todo: fix aarch64 include file lookup)
def extract_system_call_ids(unistd_64_fh):
    syscall_id_to_name = {}

    # strip 3264bit prefixes from syscall names
    for name_prefix in ['__NR_', '__NR3264_']:
        for line in unistd_64_fh.readlines():
            tokens = line.split()
            if tokens and len(tokens) == 3 and tokens[0] == '#define':
                _, s_name, s_id = tokens
                s_id = int(s_id)
                if s_name.startswith(name_prefix):
                    s_name = s_name[len(name_prefix):]
                    syscall_id_to_name[s_id] = s_name

    return syscall_id_to_name

def get_system_call_names():
    psn_dir=os.path.dirname(os.path.realpath(__file__))
    kernel_ver=platform.release().split('-')[0]

    # this probably needs to be improved for better platform support
    if platform.machine() == 'aarch64':
        unistd_64_paths = ['/usr/include/asm-generic/unistd.h']
    else:
        unistd_64_paths = [  '/usr/include/asm/unistd_64.h', '/usr/include/x86_64-linux-gnu/asm/unistd_64.h'
                           , '/usr/include/asm-x86_64/unistd.h', '/usr/include/asm/unistd.h'
                           , psn_dir+'/syscall_64_'+kernel_ver+'.h', psn_dir+'/syscall_64.h']
    
    for path in unistd_64_paths:
        try:
            with open(path) as f:
                return extract_system_call_ids(f)
        except IOError as e:
            pass

    raise Exception('unistd_64.h not found in' + ' or '.join(unistd_64_paths) + '.\n' +
                    '           You may need to "dnf install kernel-headers" or "apt-get install libc6-dev"\n') 

# syscall lookup table
syscall_id_to_name = get_system_call_names()


# task states
TASK_RUNNING           =   0x00000000
TASK_INTERRUPTIBLE     =   0x00000001
TASK_UNINTERRUPTIBLE   =   0x00000002
TASK_STOPPED           =   0x00000004
TASK_TRACED            =   0x00000008

EXIT_DEAD              =   0x00000010
EXIT_ZOMBIE            =   0x00000020
EXIT_TRACE             =   (EXIT_ZOMBIE | EXIT_DEAD)

TASK_PARKED            =   0x00000040
TASK_DEAD              =   0x00000080
TASK_WAKEKILL          =   0x00000100
TASK_WAKING            =   0x00000200
TASK_NOLOAD            =   0x00000400
TASK_NEW               =   0x00000800
TASK_RTLOCK_WAIT       =   0x00001000
TASK_FREEZABLE         =   0x00002000
TASK_FREEZABLE_UNSAFE  =   0x00004000 # depends on: IS_ENABLED(CONFIG_LOCKDEP)
TASK_FROZEN            =   0x00008000
TASK_STATE_MAX         =   0x00010000 # as of linux kernel 6.9

##define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWPN"

task_states = {
    0x00000000: "R", # "RUNNING",
    0x00000001: "S", # "INTERRUPTIBLE",
    0x00000002: "D", # UNINTERRUPTIBLE",
    0x00000004: "T", # "STOPPED",
    0x00000008: "t", # "TRACED",
    0x00000010: "X", # "EXIT_DEAD",
    0x00000020: "Z", # "EXIT_ZOMBIE",
    0x00000040: "P", # "PARKED",
    0x00000080: "dd",# "DEAD",
    0x00000100: "wk",# "WAKEKILL",
    0x00000200: "wg",# "WAKING",
    0x00000400: "I", # "NOLOAD",
    0x00000800: "N", # "NEW",
    0x00001000: "rt",# "RTLOCK_WAIT",
    0x00002000: "fe",# "FREEZABLE",
    0x00004000: "fu",# "__TASK_FREEZABLE_UNSAFE = (0x00004000 * IS_ENABLED(CONFIG_LOCKDEP))"
    0x00008000: "fo",# "FROZEN"
}


def get_task_state_name(task_state):
    if task_state == 0:
        return "R"
    if task_state & TASK_NOLOAD: # idle kthread waiting for work
        return "I"

    names = []
    for state, name in task_states.items():
        if task_state & state:
            names.append(name)

    return "+".join(names)
    

# translate uid to username
def get_username(uid):
    try:
        username = pwd.getpwuid(uid).pw_name
        return username
    except KeyError:
        return str(uid)



def print_fields(rows, columns):
    columns = [col.rstrip() for col in columns] # strip as colname might have extra spaces passed in for width/formatting
    col_widths = {}
    # column width auto-sizing
    for col in columns:
        col_length = len(col) # the col may have extra trailing spaces as a formatting directive
        max_value_length = max((len(str(row[col])) for row in rows if col in row), default=0)
        col_widths[col] = max(col_length, max_value_length)

    header1 = "=== Active Threads "
    header2 = " | ".join(f"{col:<{col_widths[col]}}" for col in columns)

    print("\n" + header1 + "=" * (len(header2) - len(header1)) + "\n")
    print(header2)
    print("-" * len(header2))

    for row in rows:
        line = " | ".join(
            f"{row[col]:>{col_widths[col]}.2f}" if col in ["samples", "avg_threads"] else f"{str(row[col]):<{col_widths[col]}}"
            if col in row else ' ' * col_widths[col] for col in columns
        )
        print(line)



def get_ustack_traces(ustack_traces, strip_args=True):
    exclusions = ['__GI___clone3']
    lines = []

    for stack_id, pid in output_ustack:
        if stack_id and stack_id >= 0:   # todo: find why we have Null/none stackids in this map
            print("stack_id")
            line = f"ustack {stack_id} ->"
            stack = list(ustack_traces.walk(stack_id))
            for addr in reversed(stack):
                func = b.sym(addr, pid).decode(DECODE_CHARSET, 'replace')
                if func not in exclusions:
                    if strip_args:
                        func = re.split('[<(]', func)[0]
                    line += "->" + (func if func != '[unknown]' else '{:x}'.format(addr))

            lines.append(line)

    return lines

def get_kstack_traces(kstack_traces):
    exclusions = ['entry_SYSCALL_64_after_hwframe', 'do_syscall_64', 'x64_sys_call', 'ret_from_fork_asm', 'ret_from_fork'
                 , '__bpf_trace_sched_switch', '__traceiter_sched_switch']
    lines = []
 
    for k, v in kstack_traces.items():
        stack_id = k.value
        if stack_id in output_kstack:
            line = f"kstack {stack_id} ->"
            if stack_id >= 0:
                stack = list(kstack_traces.walk(stack_id))
                for addr in reversed(stack):
                    func = b.ksym(addr).decode(DECODE_CHARSET, 'replace')
                    if func not in exclusions and not func.startswith('bpf_'):
                        line += "->" + b.ksym(addr).decode(DECODE_CHARSET, 'replace')

                lines.append(line)
        
    return lines


def pivot_stack_traces(traces):
    pivoted_traces = []
    for trace in traces:
        parts = trace.split("->")
        pivoted_traces.append(parts)
    
    max_length = max(len(trace) for trace in pivoted_traces)
    for trace in pivoted_traces:
        while len(trace) < max_length:
            trace.append("")
    
    return pivoted_traces

def calculate_columns(pivoted_traces, max_line_length):
    max_length = max(len(part) for trace in pivoted_traces for part in trace)
    return max(1, max_line_length // (max_length + 3))

def print_pivoted_dynamic(traces, max_line_length):
    num_traces = len(traces)
    start = 0
    
    while start < num_traces:
        end = start + 1
        while end <= num_traces:
            subset_traces = traces[start:end]
            pivoted_traces = pivot_stack_traces(subset_traces)
            num_columns = calculate_columns(pivoted_traces, max_line_length)
            
            if num_columns < end - start:
                break
            
            end += 1

        end -= 1
        subset_traces = traces[start:end]
        pivoted_traces = pivot_stack_traces(subset_traces)
        
        max_length = max(len(part) for trace in pivoted_traces for part in trace)
        
        print("-" * max_line_length)
        for row in zip(*pivoted_traces):
            print(" | ".join(f"{part:<{max_length}}" for part in row) + ' |')
        
        start = end

# def calculate_columns(pivoted_traces, max_line_length):
#     max_length = max(len(part) for trace in pivoted_traces for part in trace)
#     return max(1, max_line_length // (max_length + 3))
# 
# def print_pivoted_dynamic(traces, max_line_length):
#     num_traces = len(traces)
#     start = 0
#     
#     while start < num_traces:
#         # Get the pivoted traces for the remaining stack traces
#         pivoted_traces = pivot_stack_traces(traces[start:])
#         num_columns = calculate_columns(pivoted_traces, max_line_length)
#         
#         # Ensure at least one column
#         if num_columns == 0:
#             num_columns = 1
#         
#         end = min(start + num_columns, num_traces)
#         subset_traces = traces[start:end]
#         pivoted_traces = pivot_stack_traces(subset_traces)
#         
#         for row in zip(*pivoted_traces):
#             max_length = max(len(part) for part in row)
#             print(" | ".join(f"{part:<{max_length}}" for part in row))
#         
#         print("\n" + "-" * max_line_length + "\n")
#         start = end

# THIS is ok

# calculates the number of columns that fit max output line lenght for each output section
# def print_pivoted_dynamic(traces, max_line_length=130):
#     pivoted_traces = pivot_stack_traces(traces)
#     max_length = max(len(part) for trace in pivoted_traces for part in trace)
#     num_columns = max(1, max_line_length // (max_length + 3)) 
# 
#     num_traces = len(traces)
#     for start in range(0, num_traces, num_columns):
#         end = min(start + num_columns, num_traces)
#         subset_traces = traces[start:end]
#         pivoted_traces = pivot_stack_traces(subset_traces)
# 
#         # recalculate max columns for the current subset
#         max_length = max(len(part) for trace in pivoted_traces for part in trace)
#         num_columns = max(1, max_line_length // (max_length + 3))
# 
#         for row in zip(*pivoted_traces):
#             print(" | ".join(f"{part:<{max_length}}" for part in row))
#         print("\n" + "-"*max_line_length + "\n")


# def print_pivoted_dynamic(traces, max_line_length=130):
#     pivoted_traces = pivot_stack_traces(traces)
#     max_length = max(len(part) for trace in pivoted_traces for part in trace)
#     num_columns = max(1, max_line_length // (max_length + 3))  # Calculate number of columns that fit within the max_line_length
# 
#     num_traces = len(traces)
#     for start in range(0, num_traces, num_columns):
#         end = min(start + num_columns, num_traces)
#         subset_traces = traces[start:end]
#         pivoted_traces = pivot_stack_traces(subset_traces)
#         
#         for row in zip(*pivoted_traces):
#             print(" | ".join(f"{part:<{max_length}}" for part in row))
#         print("\n" + "-"*max_line_length + "\n")


# group by for reporting
def group_by(records, column_names):
    grouped_data = defaultdict(lambda: {'samples': 0})

    for record in records:
        key = tuple(record[col] for col in column_names)
        if key not in grouped_data:
            grouped_data[key].update({col: record[col] for col in column_names})
        grouped_data[key]['samples'] += 1

    total_records = len(records)
    grouped_list = list(grouped_data.values())

    for item in grouped_list:
        item['avg_threads'] = round(item['samples'] / total_records, 2)

    return grouped_list


# main()
signal.signal(signal.SIGPIPE, signal.SIG_DFL)

# args 
parser = argparse.ArgumentParser(description=__description__)
parser.add_argument('-x', '--xtop', action='store_true', help='Run in aggregated top-thread-activity (xtop) mode')
parser.add_argument('-g', '--group-by', metavar='csv-columns', default=DEFAULT_GROUP_BY, help='Full column list what to group by')
parser.add_argument('-G', '--append-group-by', metavar='append-csv-columns', default=None, help='List of additional columns to default cols what to group by')
parser.add_argument('-n', '--nerd-mode', action='store_true', help='Print out relevant stack traces as wide output lines')
parser.add_argument('-N', '--giant-nerd-mode', action='store_true', help='Print out relevant stack traces as stack-tiles')
parser.add_argument('-r', '--refresh', action='store_true', help='Clear screen before printing next output')
parser.add_argument('-V', '--version', action='version', version=f"%(prog)s {__version__} by {__author__} [{__url__}]", help='Show the program version and exit')
parser.add_argument('-o', '--output-dir', type=str, default=DEFAULT_PATH, help=f'Directory path where to write the output CSV files, default is {DEFAULT_PATH}')
parser.add_argument('-l', '--list', default=None, action='store_true', help='list all available columns for display and grouping')

args = parser.parse_args()

if args.list:
    for f in output_fields:
        print(f)
    sys.exit(0)

# handle -g and -G
groupby_fields = args.group_by.split(',')
groupby_fields = groupby_fields + args.append_group_by.split(',') if args.append_group_by else groupby_fields

if set(groupby_fields) - set(output_fields):
    print("Error: incorrect group by field name specified, use --list option see allowed columns")
    exit(1)

# eBPF programs have be loaded as root
if os.geteuid() != 0:
    print("Error: you need to run this command as root")
    sys.exit(1)


with open('xcapture-bpf.c', 'r') as file:
    bpf_text = file.read()

print('Loading BPF...')
b = BPF(text=bpf_text)

# Software CPU_CLOCK is useful in cloud & VM environments where perf hardware events 
# are not available, but software clocks don't measure what happens when CPUs are in 
# critical sections when most interrupts are disabled
b.attach_perf_event(ev_type=PerfType.SOFTWARE, ev_config=PerfSWConfig.CPU_CLOCK
                    , fn_name="update_cpu_stack_profile", sample_freq=1)

# get own pid so to not display it in output
mypid = os.getpid()
print(f"Ready (mypid={mypid})")

time.sleep(1.1)
# start sampling the Task State Array
tsa = b.get_table("tsa")
stackmap = b.get_table("stackmap")


while True:
    output_kstack = {} # map of stack_ids seen so far
    output_ustack = {}
    output_records = []
    sample_time = time.time()

    for sample_count in range(1,10): 

        ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
        i = tsa.items()[0]

        for i in tsa.items():
            save_record = True
            # extract python values from BPF ctypes, return '-' if there's no match
            fields_dict = {field[0]: getattr(i[1], field[0], '-') for field in i[1]._fields_}

            if fields_dict['tid'] == mypid:
                continue

            # additional fields for adding human readable info
            fields_dict['st'] = ''
            fields_dict['sch'] = '' # for scheduler nerds
            fields_dict['state_flags'] = '' # full scheduler state bitmap
            fields_dict['username'] = ''
            fields_dict['syscall'] = '' 
 
            current_syscall = syscall_id_to_name.get(fields_dict['syscall_id'], '-') if fields_dict['syscall_set'] else '-'
            in_sched_wakeup = fields_dict['in_sched_wakeup']
            in_sched_waking = fields_dict['in_sched_waking']
            is_running_on_cpu  = fields_dict['is_running_on_cpu']

            enriched_fields = {"timestamp": ts[:-3]}

            for field_name in fields_dict:
                outv = None # enriched value

                if field_name in ['state', 'st']:
                    state_suffix = ''
                    state = get_task_state_name(fields_dict['state'])
                    # for scheduling latency/preempted tasks (RQ)
                    if state == 'R' and not is_running_on_cpu:
                        state_suffix = 'Q'
                    if state in ['R', 'D'] or current_syscall == 'io_getevents':
                          # or current_syscall == 'semtimedop' and str(fields_dict['comm'], DECODE_CHARSET).startswith('oracle'): # oracle example
                        outv = state + state_suffix
                    else:
                        save_record = False
                        break

                elif field_name == 'syscall':
                    outv = current_syscall

                elif field_name == 'username':
                    outv = get_username(fields_dict['uid']) 

                elif field_name.endswith('_k'):   # kstack id
                    val = fields_dict[field_name]
                    outv = val if val else '-'
                    output_kstack[val] = True

                elif field_name.endswith("_u"):   # ustack id
                    val = fields_dict[field_name]
                    outv = val if val else '-'
                    # using pid/tgid here, address space is same for all threads in a process
                    output_ustack[val, fields_dict['pid']] = True  

                elif field_name == 'sch': 
                    # (in_sched_waking, in_sched_wakeup, is_running_on_cpu)
                    outv  = '-' if in_sched_waking else '_'
                    outv += '-' if in_sched_wakeup else '_'
                    outv += '-' if is_running_on_cpu else '_'

                else:
                    val = fields_dict[field_name]
                    if isinstance(val, bytes):
                        outv = str(val, DECODE_CHARSET)
                    else:
                        outv = str(val)
                    
                enriched_fields[field_name] = outv

            if save_record:
                output_records.append(enriched_fields)

        elapsed_time = time.time() - sample_time
        sleep_time = max(0, 5 - elapsed_time)
        time.sleep(sleep_time)


    if output_records:
        # printing stacktiles first, so the task state info is in the bottom of terminal output
        (term_width, term_height) = shutil.get_terminal_size()

        print_pivoted_dynamic(get_kstack_traces(stackmap), max_line_length=term_width)
        print()

        print_pivoted_dynamic(get_ustack_traces(stackmap), max_line_length=term_width)
        print()

        # regular nerd mode
        #for s in get_kstack_traces(stackmap): print(s)
        #for s in get_ustack_traces(stackmap): print(s)


        grouped_list = group_by(output_records, groupby_fields) # a new field "samples" shows up (count(*))
        ordered_aggr = sorted(grouped_list, key=lambda x: x['samples'], reverse=True)
        print_fields(ordered_aggr, ['samples', 'avg_threads'] + groupby_fields)
        
        print()
        print()


# That's all, folks!


# platform.release().split('-')[0] + " Linux " + platform.machine() + " " 
# def print_header(fieldlist):
#     print('TIMESTAMP,' + ','.join(fieldlist).upper())
