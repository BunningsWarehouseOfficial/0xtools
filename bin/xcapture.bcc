/*
 *  0x.Tools xcapture.bcc v0.5 beta
 *  Sample Linux thread activity using eBPF [0x.tools]
 *
 *  Copyright 2019-2024 Tanel Poder
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License along
 *  with this program; if not, write to the Free Software Foundation, Inc.,
 *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
 *
 *  SPDX-License-Identifier: GPL-2.0-or-later
 *
 */

#include <uapi/linux/ptrace.h>
#include <linux/sched.h>
#include <linux/types.h>
#include <linux/uidgid.h>
#include <linux/cred.h>

#ifdef BCC_SEC
#define __BCC__
#endif


BPF_STACK_TRACE(kstackmap, 131072);
BPF_STACK_TRACE(ustackmap, 131072);

struct thread_state_t {
    u32 state;
    u32 tid;  // thread (task) ID 
    u32 pid;  // the process ID (thread group leader ID)

    u32 effective_uid;
    s32 login_uid; // using s32 as __kuid_val() returns -1 on lookup failure

    char cmdline[64]; // just first bytes for now
    char comm[TASK_COMM_LEN];

    u16 syscall_id; // unsigned as we switch the value to negative once the syscall is done, to see the last syscall
    unsigned long syscall_arg0;
    unsigned long syscall_arg1;
    unsigned long syscall_arg2;
    unsigned long syscall_arg3;
    unsigned long syscall_arg4;
    unsigned long syscall_arg5;

    s32 offcpu_ustack; 
    s32 offcpu_kstack; 
    s32 profile_ustack;
    s32 profile_kstack;
    s32 syscall_ustack;
    //s32 sched_waker_kstack;
    //s32 sched_waker_ustack;

    bool in_sched_wakeup;
    u32 waker_tid;

    // app-specific context
    //s32 oracle_wait_event;

    // debug and internal use by python frontend
    bool syscall_set; // 0 means that syscall probe has not fired yet for this task, so don't use syscall_id
    s16 probe;
    s16 probe_before;
};


// not using BPF_F_NO_PREALLOC here for now, trading some kernel memory for better performance
BPF_HASH(tsa, u32, struct thread_state_t, 16384);

TRACEPOINT_PROBE(raw_syscalls, sys_enter) {
    struct thread_state_t t_empty = {};

    u32 tid = bpf_get_current_pid_tgid() & 0xffffffff;
    u32 pid = bpf_get_current_pid_tgid() >> 32;
    struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();
    
    struct thread_state_t *t = tsa.lookup_or_try_init(&tid, &t_empty);
    if (!t) return 0;

    t->probe = 1; // "sys_enter"
    t->tid = tid;
    t->pid = pid;
    bpf_get_current_comm(&t->comm, sizeof(t->comm));
    t->state = curtask->__state;

    t->syscall_set  = 1;
    t->syscall_id   = args->id,
    t->syscall_arg0 = args->args[0];
    t->syscall_arg1 = args->args[1];
    t->syscall_arg2 = args->args[2];
    t->syscall_arg3 = args->args[3];
    t->syscall_arg4 = args->args[4];
    t->syscall_arg5 = args->args[5];
    t->syscall_ustack = ustackmap.get_stackid(args, BPF_F_USER_STACK | BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);

    tsa.update(&tid, t);
    return 0;
} // raw_syscalls:sys_enter


TRACEPOINT_PROBE(raw_syscalls, sys_exit) {
    
    u32 tid = bpf_get_current_pid_tgid() & 0xffffffff;
    u32 pid = bpf_get_current_pid_tgid() >> 32;
    struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();

    struct thread_state_t t_empty = {};
    struct thread_state_t *t = tsa.lookup_or_try_init(&tid, &t_empty);
    if (!t) return 0;

    t->probe = 2; // "sys_exit";
    t->tid = tid;
    t->pid = pid;
    bpf_get_current_comm(&t->comm, sizeof(t->comm));
    t->state = curtask->__state;
    t->syscall_id = t->syscall_id * -1; // switch the syscall_id to its negative value on exit

    tsa.update(&tid, t);

    return 0;
} // raw_syscalls:sys_exit


// sampling profiling of on-CPU threads (python frontend uses perf event with freq=1)
// update the stack id of threads currently running on (any) CPU
int update_cpu_stack_profile(struct bpf_perf_event_data *ctx) {

    u32 tid = bpf_get_current_pid_tgid() & 0xffffffff;

    // ignore tid 0 - kernel cpuidle
    if (tid) {
        u32 pid = bpf_get_current_pid_tgid() >> 32;
        struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();

        struct thread_state_t t_empty = {};
        struct thread_state_t *t = tsa.lookup_or_try_init(&tid, &t_empty);
        if (!t) return 0;

        t->probe = 5; // "cpu_stack_profile";
        t->tid = tid;
        t->pid = pid;
        t->state = curtask->__state;

        t->profile_ustack = ustackmap.get_stackid(ctx, BPF_F_USER_STACK | BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);
        t->profile_kstack = kstackmap.get_stackid(ctx, BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);

        tsa.update(&tid, t);
    }

    return 0;
};


// Context enrichment example (kernel): tasks waiting in the CPU runqueue
// tracepoint:sched:sched_wakeup,sched_wakeup_new (will add sched_waker at some point too)
TRACEPOINT_PROBE(sched, sched_wakeup) {

    struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();
    struct thread_state_t t_empty = {};

    u32 tid_woken = args->pid;
 
    struct thread_state_t *t_being_waked_up = tsa.lookup_or_try_init(&tid_woken, &t_empty);
    if (!t_being_waked_up) return 0;

    t_being_waked_up->probe = 6; // "sched_wakeup";
    t_being_waked_up->in_sched_wakeup = 1;
    t_being_waked_up->tid = tid_woken;          // this guy is being woken up
    t_being_waked_up->waker_tid = curtask->pid; // this is who wakes that guy up

    tsa.update(&tid_woken, t_being_waked_up);

    return 0;
}

// duplicate function (can be fixed in python code with attach_tracepoint)
TRACEPOINT_PROBE(sched, sched_wakeup_new) {

    struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();
    u32 tid_woken = args->pid;

    struct thread_state_t t_empty = {};
    struct thread_state_t *t_being_waked_up = tsa.lookup_or_try_init(&tid_woken, &t_empty);
    if (!t_being_waked_up) return 0;

    t_being_waked_up->probe = 7; // "sched_wakeup_new";
    t_being_waked_up->in_sched_wakeup = 1;
    t_being_waked_up->tid = tid_woken;          // this guy is being woken up
    t_being_waked_up->waker_tid = curtask->pid; // this is who wakes that guy up

    tsa.update(&tid_woken, t_being_waked_up);

    return 0;
}

// "next" is about to be put on CPU, "prev" goes off-CPU
RAW_TRACEPOINT_PROBE(sched_switch) {

    s32 prev_tid; // task
    s32 prev_pid; // tgid
    s32 next_tid; // task
    s32 next_pid; // tgid

    // from https://github.com/torvalds/linux/blob/master/include/trace/events/sched.h (sched_switch trace event)
    bool *preempt; // = (bool)ctx->args[0];
    struct task_struct *prev = (struct task_struct *)ctx->args[1];
    struct task_struct *next = (struct task_struct *)ctx->args[2];
    unsigned int prev_state; // = (unsigned int)ctx->args[3];
    
    prev_tid = prev->pid;
    prev_pid = prev->tgid;
    next_tid = next->pid;
    next_pid = next->tgid;
    prev_state = ctx->args[3];

    struct thread_state_t t_empty_prev = {};
    struct thread_state_t t_empty_next = {};

    // we don't want to capture/report the previous cpuidle "task" during actual task wakeups (tid 0)
    if (prev_tid) {
        struct thread_state_t *t_prev = tsa.lookup_or_try_init(&prev_tid, &t_empty_prev);
        if (!t_prev) return 0;

        t_prev->tid = prev_tid;
        t_prev->pid = prev_pid;
        t_prev->in_sched_wakeup = 0;
        bpf_probe_read_str(t_prev->comm, sizeof(t_prev->comm), prev->comm); // BCC allows this syntax

        t_prev->offcpu_ustack = ustackmap.get_stackid(ctx, BPF_F_USER_STACK | BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);
        t_prev->offcpu_kstack = kstackmap.get_stackid(ctx, BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);

        // debug
        t_prev->probe_before = t_prev->probe;
        t_prev->probe = 8; // "sched_switch(prev)";
        t_prev->state = prev_state; // prev_state passed in as an arg to sched_switch probe

        
        tsa.update(&prev_tid, t_prev);
    }

    // we don't want to capture/report the cpuidle "task" (tid 0) when CPU goes to cpuidle
    if (next_tid) {
        struct thread_state_t *t_next = tsa.lookup_or_try_init(&next_tid, &t_empty_next);
        if (!t_next) return 0;

        t_next->tid = next_tid;
        t_next->pid = next_pid;
        bpf_probe_read_str(t_next->comm, sizeof(t_next->comm), next->comm); // BCC allows this syntax
        t_next->in_sched_wakeup = 0;
        t_next->login_uid = next->loginuid.val; //__kuid_val(next->loginuid);
        t_next->effective_uid = __kuid_val(next->cred->euid);

        // debug
        t_next->probe_before = t_next->probe;
        t_next->probe = 9; // "sched_switch(next)";
        t_next->state = next->__state;

        tsa.update(&next_tid, t_next);
    }

    return 0;
}

// remove the hashmap element on user thread exit
TRACEPOINT_PROBE(sched, sched_process_exit) {

    u32 tid_exiting = args->pid;
    tsa.delete(&tid_exiting);

    return 0;
}

// remove the hashmap element on kthread exit (should really wait until sched_kthread_stop_ret)
TRACEPOINT_PROBE(sched, sched_kthread_stop) {

    u32 tid_exiting = args->pid;
    tsa.delete(&tid_exiting);

    return 0;
}

// vim:syntax=c
