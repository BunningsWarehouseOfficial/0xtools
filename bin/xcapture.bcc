/*
 *  0x.Tools xcapture.bcc v0.4 beta
 *  Sample Linux thread activity using eBPF [0x.tools]
 *
 *  Copyright 2019-2023 Tanel Poder
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License along
 *  with this program; if not, write to the Free Software Foundation, Inc.,
 *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
 *
 *  SPDX-License-Identifier: GPL-2.0-or-later
 *
 */

#include <uapi/linux/ptrace.h>
#include <linux/sched.h>
#include <linux/log2.h>

#ifdef BCC_SEC
#define __BCC__
#endif


BPF_STACK_TRACE(kstackmap, 131072);
BPF_STACK_TRACE(ustackmap, 131072);
BPF_PERF_OUTPUT(events);

struct thread_state_t {
    u32 state;
    u32 tid;  // thread (task) ID 
    u32 pid;  // the process ID (thread group leader ID)
    //u32 uid;
    //u32 gid;
    //char cmdline[64]; // just first bytes for now
    char comm[TASK_COMM_LEN];

    int  syscall_id;
    long offcpu_ustack; 
    long offcpu_kstack; 
    // consolidate these to a single struct or union for lower BPF map mem usage (?)
    unsigned long syscall_arg0;
    unsigned long syscall_arg1;
    unsigned long syscall_arg2;
    unsigned long syscall_arg3;
    unsigned long syscall_arg4;
    unsigned long syscall_arg5;

    long profile_ustack;
    long profile_kstack;
    long syscall_ustack;
    int  in_sched_wakeup;
    //long sched_waker_kstack;
    //long sched_waker_ustack;
    //u32 oracle_wait_event;
    int syscall_set; // 0 means that syscall probe has not fired yet for this task, so don't use syscall_id

    // debug
    int probe;
    int probe_before;
    int err;
};


// not using BPF_F_NO_PREALLOC here for now, trading some kernel memory for better performance
BPF_HASH(tsa, u32, struct thread_state_t, 65536);

TRACEPOINT_PROBE(raw_syscalls, sys_enter) {
    struct thread_state_t t_empty = {};

    u32 tid = bpf_get_current_pid_tgid() & 0xffffffff;
    u32 pid = bpf_get_current_pid_tgid() >> 32;
    struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();
    
    struct thread_state_t *t = tsa.lookup_or_try_init(&tid, &t_empty);
    if (!t) return 0;

    t->probe = 1; // "sys_enter"
    t->tid = tid;
    t->pid = pid;
    bpf_get_current_comm(&t->comm, sizeof(t->comm));
    t->state = curtask->__state;

    t->syscall_set  = 1;
    t->syscall_id   = args->id,
    t->syscall_arg0 = args->args[0];
    t->syscall_arg1 = args->args[1];
    t->syscall_arg2 = args->args[2];
    t->syscall_arg3 = args->args[3];
    t->syscall_arg4 = args->args[4];
    t->syscall_arg5 = args->args[5];
    t->syscall_ustack = ustackmap.get_stackid(args, BPF_F_USER_STACK | BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);

    tsa.update(&tid, t);
    return 0;
} // raw_syscalls:sys_enter


TRACEPOINT_PROBE(raw_syscalls, sys_exit) {
    
    u32 tid = bpf_get_current_pid_tgid() & 0xffffffff;
    u32 pid = bpf_get_current_pid_tgid() >> 32;
    struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();

    struct thread_state_t t_empty = {};
    struct thread_state_t *t = tsa.lookup_or_try_init(&tid, &t_empty);
    if (!t) return 0;

    t->probe = 2; // "sys_exit";
    t->tid = tid;
    t->pid = pid;
    bpf_get_current_comm(&t->comm, sizeof(t->comm));
    t->state = curtask->__state;
    t->syscall_id = t->syscall_id * -1; // switch the syscall_id to its negative value on exit

    tsa.update(&tid, t);

    return 0;
}


// kprobe:schedule - thread requests going off CPU
// by the time schedule() is called, the caller has set the new task state
//int update_offcpu_schedule(struct pt_regs *ctx) {
//
//    u32 tid = bpf_get_current_pid_tgid() & 0xffffffff;
//    u32 pid = bpf_get_current_pid_tgid() >> 32;
//    struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();
//
//    struct thread_state_t t_empty = {};
//    struct thread_state_t *t = tsa.lookup_or_try_init(&tid, &t_empty);
//    if (!t) return 0;
//
//    t->probe = 3; // "offcpu_schedule";
//    t->tid = tid;
//    t->pid = pid;
//    bpf_get_current_comm(&t->comm, sizeof(t->comm));
//    t->state = curtask->__state;
//    t->offcpu_ustack = ustackmap.get_stackid(ctx, BPF_F_USER_STACK | BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);
//    t->offcpu_kstack = kstackmap.get_stackid(ctx, BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);
//
//    tsa.update(&tid, t);
//
//    return 0;
//};

// kprobe:finish_task_switch* - thread has been put back on CPU by scheduler
// newer kernels have the "isra" version of this function name, thus the * wildcard
// int update_offcpu_finish_task_switch(struct pt_regs *ctx, struct task_struct *prevtask) {
// int update_offcpu_finish_task_switch(struct pt_regs *ctx) {
// 
//     struct task_struct *prevtask = (struct task_struct *)ctx;
// 
//     pid_t prev_tid = prevtask->pid; 
//     pid_t prev_pid = prevtask->tgid;
//     u32 prev_state = prevtask->__state;
// 
//     // bpf_probe_read_kernel(&prev_tid, sizeof(prevtask->pid), &prevtask->pid);
//     // bpf_probe_read_kernel(&prev_pid, sizeof(prevtask->tgid), &prevtask->tgid);
// 
//     struct thread_state_t t_empty = {};
//     struct thread_state_t *t = tsa.lookup_or_try_init(&prev_tid, &t_empty);
//     if (!t) return 0;
// 
//     //t->probe = 4; // "offcpu_finish_task_switch";
//     t->tid = prev_tid;  // prev_tid;
//     t->pid = prev_pid; // prev_pid;
//     t->state = prev_state;
// 
//     tsa.update(&prev_tid, t);
// 
//     return 0;
// };


// sampled profiling of on-CPU threads - profile:hz:1
// update the stack id of threads currently running on (any) cpu

int update_cpu_stack_profile(struct bpf_perf_event_data *ctx) {

    u32 tid = bpf_get_current_pid_tgid() & 0xffffffff;

    // ignore tid 0 - kernel cpuidle
    if (tid) {
        u32 pid = bpf_get_current_pid_tgid() >> 32;
        struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();

        struct thread_state_t t_empty = {};
        struct thread_state_t *t = tsa.lookup_or_try_init(&tid, &t_empty);
        if (!t) return 0;

        t->probe = 5; // "cpu_stack_profile";
        t->tid = tid;
        t->pid = pid;
        t->state = curtask->__state;

        t->profile_ustack = ustackmap.get_stackid(ctx, BPF_F_USER_STACK | BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);
        t->profile_kstack = kstackmap.get_stackid(ctx, BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);

        tsa.update(&tid, t);
    }

    return 0;
};


// Context enrichment example (kernel): tasks waiting in the CPU runqueue
// tracepoint:sched:sched_wakeup,sched_wakeup_new 
TRACEPOINT_PROBE(sched, sched_wakeup) {

    //struct task_struct *curtask = (struct task_struct *) bpf_get_current_task();
    struct thread_state_t t_empty = {};

    u32 tid_woken = args->pid;
 
    struct thread_state_t *t_being_waked_up = tsa.lookup_or_try_init(&tid_woken, &t_empty);
    if (!t_being_waked_up) return 0;

    t_being_waked_up->probe = 6; // "sched_wakeup";
    t_being_waked_up->in_sched_wakeup = 1;

    tsa.update(&tid_woken, t_being_waked_up);

    return 0;
}

// duplicate function (can be fixed in python code with attach_tracepoint)
TRACEPOINT_PROBE(sched, sched_wakeup_new) {

    u32 tid_woken = args->pid;

    struct thread_state_t t_empty = {};
    struct thread_state_t *t_being_waked_up = tsa.lookup_or_try_init(&tid_woken, &t_empty);
    if (!t_being_waked_up) return 0;

    t_being_waked_up->probe = 7; // "sched_wakeup_new";
    t_being_waked_up->tid = tid_woken;
    t_being_waked_up->in_sched_wakeup = 1;

    tsa.update(&tid_woken, t_being_waked_up);

    return 0;
}

// target is about to be put on CPU
RAW_TRACEPOINT_PROBE(sched_switch) {

    s32 prev_tid; // task
    s32 prev_pid; // tgid

    s32 next_tid; // task
    s32 next_pid; // tgid

    // from https://github.com/torvalds/linux/blob/master/include/trace/events/sched.h (sched_switch trace event)
    bool *preempt; // = (bool)ctx->args[0];
    struct task_struct *prev = (struct task_struct *)ctx->args[1];
    struct task_struct *next = (struct task_struct *)ctx->args[2];
    unsigned int prev_state; // = (unsigned int)ctx->args[3];
    
    // prev_tid = prev->pid;
    // prev_pid = prev->tgid;
    // next_tid = next->pid;
    // next_pid = next->tgid;

    int err = 0;

    err += bpf_probe_read_kernel(&prev_tid, sizeof(prev->pid), &prev->pid);
    err += bpf_probe_read_kernel(&prev_pid, sizeof(prev->tgid), &prev->tgid);
    err += bpf_probe_read_kernel(&next_tid, sizeof(next->pid), &next->pid);
    err += bpf_probe_read_kernel(&next_pid, sizeof(next->tgid), &next->tgid);
    err += bpf_probe_read_kernel(&prev_state, sizeof(prev_state), &ctx->args[3]); // 4th arg (arg3) is prev_state

    struct thread_state_t t_empty_prev = {};
    struct thread_state_t t_empty_next = {};

    // we don't want to capture/report the previous cpuidle state during actual task wakeups
    if (prev_tid) {
        struct thread_state_t *t_prev = tsa.lookup_or_try_init(&prev_tid, &t_empty_prev);
        if (!t_prev) return 0;

        t_prev->probe_before = t_prev->probe;
        t_prev->probe = 8; // "sched_switch(prev)";
        t_prev->tid = prev_tid;
        t_prev->pid = prev_pid;
        t_prev->in_sched_wakeup = 0;
        t_prev->state = prev_state; // prev_state passed in as an arg to sched_switch probe
        t_prev->err = err;

        t_prev->offcpu_ustack = ustackmap.get_stackid(ctx, BPF_F_USER_STACK | BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);
        t_prev->offcpu_kstack = kstackmap.get_stackid(ctx, BPF_F_REUSE_STACKID | BPF_F_FAST_STACK_CMP);
        
        tsa.update(&prev_tid, t_prev);
    }

    // we don't want to capture/report kernel state when CPU goes to cpuidle
    if (next_tid) {
        struct thread_state_t *t_next = tsa.lookup_or_try_init(&next_tid, &t_empty_next);
        if (!t_next) return 0;

        t_next->probe_before = t_next->probe;
        t_next->probe = 9; // "sched_switch(next)";
        t_next->tid = next_tid;
        t_next->pid = next_pid;
        t_next->in_sched_wakeup = 0;
        t_next->state = next->__state;
        t_next->err = err;

        tsa.update(&next_tid, t_next);
    }

    return 0;
}

// remove the hashmap element on user thread exit
TRACEPOINT_PROBE(sched, sched_process_exit) {

    u32 tid_exiting = args->pid;
    tsa.delete(&tid_exiting);

    return 0;
}

// remove the hashmap element on kthread exit (should really wait until sched_kthread_stop_ret)
TRACEPOINT_PROBE(sched, sched_kthread_stop) {

    u32 tid_exiting = args->pid;
    tsa.delete(&tid_exiting);

    return 0;
}


// random stuff and junk

// periodic sampling
// int emit_periodic_sample() {
//     u64 ts = bpf_ktime_get_ns();
//     return 0;
// }

// BPF_F_SKIP_FIELD_MASK. The next bits can be used to set a combination of the following flags:
// BPF_F_USER_STACK       Collect a user space stack instead of a kernel stack.
// BPF_F_FAST_STACK_CMP   Compare stacks by hash only.
// BPF_F_REUSE_STACKID    If two different stacks hash into the same stackid, discard the old one.

// vim:syntax=c
